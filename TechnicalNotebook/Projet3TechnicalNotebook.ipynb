{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uoyav_d-V1s"
   },
   "source": [
    "# Projet préparez des données pour un organisme de santé publique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo43CJ2vH0pB"
   },
   "outputs": [],
   "source": [
    "# To run the notebook\n",
    "%pip install jupyter\n",
    "\n",
    "# To draw plots\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "# To draw plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# To manipulate dataFrames\n",
    "%pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "# To use quick functions (mainly on arrays)\n",
    "%pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "# To plot prettiers graphs simpler\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# Allow to omit the warnings\n",
    "%pip install warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# To print dataframes in a nice way\n",
    "%pip install dataframe_image\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# To be able to encode dataframe to display them as pictures\n",
    "%pip install unicodedata\n",
    "import unicodedata\n",
    "\n",
    "# To use widgets to interact with the notebook\n",
    "%pip install ipywidgets\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# To use data science models\n",
    "%pip install sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To make reports\n",
    "%pip install scipy\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/parcours-data-scientist/P2/fr.openfoodfacts.org.products.csv.zip'\n",
    "\n",
    "# Download file from url\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(url, 'fr.openfoodfacts.org.products.csv.zip')\n",
    "\n",
    "# Unzip file\n",
    "import zipfile\n",
    "with zipfile.ZipFile('fr.openfoodfacts.org.products.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "data = pd.read_csv('./fr.openfoodfacts.org.products.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkRE-O1zydZZ"
   },
   "source": [
    "# Data shape analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-9R7rcvydZZ"
   },
   "outputs": [],
   "source": [
    "# Deep copy of the original dataframe\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yu8DsyWbMozZ",
    "outputId": "0056898c-1437-4946-f619-6222c6326f4b"
   },
   "outputs": [],
   "source": [
    "# Quick description of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "UGSOhV5Nd4wf",
    "outputId": "fe0c74cf-20ef-4bd2-cd8e-5f3806ca2f21"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sm7pLmBSyXz"
   },
   "source": [
    "## Analyse des types de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "QekiMTiRJTzw",
    "outputId": "29303ffe-e2fb-4e84-a8c3-419d4104b612"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row',18)\n",
    "graph = df.dtypes.value_counts().plot.pie(title='Analyse de la proportion des types de données',autopct='%1.0f%%',).axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyazGveeSu34"
   },
   "source": [
    "## Data representation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpO3rOkxJ9a2"
   },
   "outputs": [],
   "source": [
    "# For better understanding of the dataset completion, we try to display the arrangement of the missing values by using a heatmap\n",
    "\n",
    "# Very heavy to compute\n",
    "# sns.set()\n",
    "# ax = plt.axes()\n",
    "# sns.heatmap(df.isna(), cbar=False, ax = ax)\n",
    "# ax.set_title('Complétion des données par colonne')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV7o5Md2Sdaw"
   },
   "source": [
    "## Missing values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WC9Nq61Ku9F",
    "outputId": "9d946825-7819-4791-8e06-be6eb114c53d"
   },
   "outputs": [],
   "source": [
    "# After the visualization, we try to mesure the missing values\n",
    "pd.set_option('display.max_rows', None)\n",
    "proportion = 100- (round(df.isna().sum()*100/df.shape[0],2)).sort_values(ascending=True)\n",
    "print('Completion value rate by columns:')\n",
    "print(proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of the missing values analysis\n",
    "\n",
    "* Most of the columns are mostly filled with values\n",
    "* We cannot find if this dataset is a byproduct of multiple other ones as there are not multiple groups of columns with the sate completion rate\n",
    "* Most of the columns could be worked with, as the less complete one still contains 15% of data (85% of missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-L17nJZLKqAx",
    "outputId": "445d3e93-8c4c-4193-d07b-1097dbbc3ac4"
   },
   "outputs": [],
   "source": [
    "# Usable data\n",
    "pd.set_option('display.max_rows', 20)\n",
    "seuil = 70 # Arbitrairy threshold to determine if a column is usable\n",
    "proportion[proportion > seuil].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to focus on columns that represent weight per 100g as they seem to have a great completion rate and seem relevant and consitent enough for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLDiRkSFhrqb"
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHRSjx-FwH62"
   },
   "source": [
    "## Duplicates and null identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dataset, we can see that the `code` columns is not unique as some of them contains duplicate.\n",
    "We can also predict that this identifier was computed from the column `url` and converted to an int.\n",
    "e.g. : \n",
    "| url (str)                                                | identifier (str) | code (int) |\n",
    "|----------------------------------------------------------|------------------|------------|\n",
    "| http://world-fr.openfoodfacts.org/produit/0000000003087/ | 0000000003087    | 3087       |\n",
    "| http://world-fr.openfoodfacts.org/produit/0000000002867/ | 0000000002867    | 2867(error)|\n",
    "| http://world-fr.openfoodfacts.org/produit/000002867/     | 000002867        | 2867(error)|\n",
    "\n",
    "So we decided to use the column `url` as an identifier and to compute any missing value in the column `code` from the column `url` (and vice versa if `url` is missing and not `code`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8CgqFrfwKIK",
    "outputId": "078097b0-c808-40f4-99a6-5f061a68422f"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_nan_code = pd.isna(df['code'])\n",
    "df_nan_url = pd.isna(df['url'])\n",
    "df_code_possible = df[df_nan_code != df_nan_url]\n",
    "\n",
    "print(f\"There are {df_code_possible.shape[0]} rows where we can compute either the code or url columns.\")\n",
    "\n",
    "# The purpose of this tool is to redirect the user to the openfoodfacts website, if the url is missing and we cannot reconstruct it from the code, we choose to remove the row.\n",
    "df = df.drop(df[df_nan_code].index)\n",
    "\n",
    "# Ultimately, even if the code columns contains duplicates, as we choose to use the url column as Id, this is not useful anymore to clean the code column.\n",
    "df_duplicated_url = df[df['url'].duplicated()].sort_values('url')\n",
    "print(f\"There are {df_duplicated_url.shape[0]} rows where urls are duplicated. We deleted them.\")\n",
    "\n",
    "df = df.drop(df_duplicated_url.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKqdvoHJAAEd"
   },
   "source": [
    "## Outliers selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2V660dSAC8F"
   },
   "source": [
    "In this chapter, we will focus on selection of outliers. To remove them from the dataset.\n",
    "Mainly, we will sum every 'on 100g' columns to see if it sums above 100g, which will detect one or multiple oultiers. If this is the case, we cannot determine which one is the outlier and we must delete the entire row.\n",
    "\n",
    "Another problem is that, most of the 'on 100g' columns are included in others. For exemple, it would be an error to sum 'saturated_fat_100g' and 'fat_100g' as 'saturated_fat_100g' is included in 'fat_100g'.\n",
    "We must try to understand which columns contains the others.\n",
    "For this exercice, I needed to do a lot of reasearch on biochemistry and nutrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to represent columns with a dictionnary because it is a classic object, easy to manipulate and can represent the imbrication of columns.\n",
    "For exemple, if we have the following dictionnary :\n",
    "```\n",
    "a --> a1\n",
    "  --> a2\n",
    "b --> b1\n",
    "  --> b2\n",
    "c\n",
    "```\n",
    "We would have all of this combinations possible\n",
    "1. `a1 + a2 + b1 + b2 + c`\n",
    "2. `a1 + a2 + b  + c`\n",
    "3. `a + b1 + b2  + c `\n",
    "4. `a + b  + c`\n",
    "\n",
    "Here, we have a dictionnary with a width of 3 (a, b, c) and a depth of 2 (a1, a2).\n",
    "It gives us a total of 2^3 = 8 combinations. In reality, we have four different because we \"shorten\" c.\n",
    "\n",
    "For a k-width, i-depth dicttionnary, we have i^k combinations. This number can grow so fast and we have so many columns that we decided only to sum 'by levels'.\n",
    "This means that in our case, we will only have i(=2) combinations, the first one for the first level (a + b + c) and the second one for the second level (a1 + a2 + b1 + b2 + c).\n",
    "But it also means that if a branch doesn't go down to the deepest level, we have to simulate a deeper level by keeping the same value (as we did for c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFQjpDzjCWt_"
   },
   "outputs": [],
   "source": [
    "# Using our reasearch, this is the dictionnary we came with\n",
    "\n",
    "dict_feature_combinations = {\n",
    "    'fat_100g':\n",
    "              {\n",
    "              'cholesterol_100g':{},\n",
    "              'saturated-fat_100g':{\n",
    "                                    'caprylic-acid_100g':{},\n",
    "                                    'lauric-acid_100g':{},\n",
    "                                    'myristic-acid_100g':{},\n",
    "                                    'palmitic-acid_100g':{},\n",
    "                                    'stearic-acid_100g':{},\n",
    "                                    'arachidic-acid_100g':{},\n",
    "                                    'behenic-acid_100g':{},\n",
    "                                    'lignoceric-acid_100g':{},\n",
    "                                    'cerotic-acid_100g':{},\n",
    "                                    'montanic-acid_100g':{},\n",
    "                                    'melissic-acid_100g':{},\n",
    "                                    'butyric-acid_100g':{},\n",
    "                                    'caproic-acid_100g':{},\n",
    "                                    'capric-acid_100g':{}\n",
    "                                  },\n",
    "              'monounsaturated-fat_100g':{\n",
    "                                          'omega-9-fat_100g':{\n",
    "                                                            'oleic-acid_100g':{},\n",
    "                                                            'elaidic-acid_100g':{},\n",
    "                                                            'gondoic-acid_100g':{},\n",
    "                                                            'mead-acid_100g':{},\n",
    "                                                            'erucic-acid_100g':{},\n",
    "                                                            'nervonic-acid_100g':{}\n",
    "                                                            }\n",
    "                                        },\n",
    "              'polyunsaturated-fat_100g':{\n",
    "                                        'omega-3-fat_100g':{\n",
    "                                                          'alpha-linolenic-acid_100g':{},\n",
    "                                                          'eicosapentaenoic-acid_100g':{},\n",
    "                                                          'docosahexaenoic-acid_100g':{}\n",
    "                                                          },\n",
    "                                        'omega-6-fat_100g':{\n",
    "                                                          'linoleic-acid_100g':{},\n",
    "                                                          'arachidonic-acid_100g':{},\n",
    "                                                          'gamma-linolenic-acid_100g':{},\n",
    "                                                          'dihomo-gamma-linolenic-acid_100g':{}\n",
    "                                                          }\n",
    "                                        },\n",
    "              'trans-fat_100g':{}\n",
    "              },\n",
    "    'sugars_100g':\n",
    "                {\n",
    "                    'carbohydrates_100g':{\n",
    "                                        'sucrose_100g':{},\n",
    "                                        'glucose_100g':{},\n",
    "                                        'fructose_100g':{},\n",
    "                                        'lactose_100g':{},\n",
    "                                        'maltose_100g':{}\n",
    "                    }\n",
    "\n",
    "                },\n",
    "    'proteins_100g':{\n",
    "                    'casein_100g':{},\n",
    "                    }\n",
    "    }\n",
    "\n",
    "\n",
    "# This function can either return every string of a dictionnary of return every columns for every levels (depending of the choosen option)\n",
    "def multi_purpose_function(dico, option):\n",
    "  # Option 1: get_every_string_of_dict\n",
    "  # Option 2: get_levels_features\n",
    "\n",
    "  parent  =''\n",
    "\n",
    "  level_list = [set(dico.keys())]\n",
    "  parent_list = [set()]\n",
    "  string_list = list(dico.keys())\n",
    "  # Prends toutes les clefs niveau 1\n",
    "  keys = list(dico.keys())\n",
    "  # Tant que la boite à clef à explorer n'est pas vide, on continue à explorer\n",
    "  while (keys != []):\n",
    "    # Pour chaque clef de la boite à clefs\n",
    "    for key in keys:\n",
    "      # On explore\n",
    "\n",
    "      # On décompose la clef pour avoir le multi level : On a une clef du type lvl1;lvl2;lvl3...\n",
    "      sublevel = dico\n",
    "      string_to_print = ''\n",
    "      level = 0\n",
    "      for key_level in key.split(';'):\n",
    "        string_to_print += '--'\n",
    "        level += 1\n",
    "        sublevel = sublevel[key_level]\n",
    "        if level >= len(level_list):\n",
    "          level_list.append(set())\n",
    "        if level >= len(parent_list):\n",
    "          parent_list.append(set())\n",
    "        \n",
    "\n",
    "      # On retourne toutes les clefs (sans les dict([]).keys())\n",
    "      ajout_list = False\n",
    "      prochaines_clefs = list(sublevel.keys())\n",
    "      if len(prochaines_clefs) != 0:\n",
    "        parent = prochaines_clefs[0] \n",
    "      for key2 in prochaines_clefs:\n",
    "        keys.append(f\"{key};{key2}\")\n",
    "        string_list.append(key2)\n",
    "        level_list[level].add(key2)\n",
    "\n",
    "        if sublevel[key2] != dict([]):\n",
    "          level_list[level].add(key2)\n",
    "        else:\n",
    "          parent_list[level].add(key2)\n",
    "\n",
    "      # On retire la clef explorée de la boîte à clef\n",
    "      keys.remove(key)\n",
    "  \n",
    "  for i in range(1,level):\n",
    "    parent_list[i] = parent_list[i].union(parent_list[i-1])\n",
    "    level_list[i] = level_list[i].union(parent_list[i-1])\n",
    "\n",
    "\n",
    "  if option == 1:\n",
    "    return string_list\n",
    "  if option == 2:\n",
    "    level_list.pop()\n",
    "    return level_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSklLdU3Ardg"
   },
   "outputs": [],
   "source": [
    "per_100g_features_list = []\n",
    "\n",
    "# Get every 'for 100g' column\n",
    "for index in data.columns:\n",
    "  if '100' in index and  data[index].dtypes == 'float64': \n",
    "    per_100g_features_list.append(index)\n",
    "\n",
    "# Here we delete the columns that doesn't seem to fit (most of them are just not weights so no possibility to sum them)\n",
    "\n",
    "not_weight_on_100g_columns = [\n",
    "                        'energy_100g',\n",
    "                        'energy-from-fat_100g',\n",
    "                        'carbon-footprint_100g',\n",
    "                        'nutrition-score-fr_100g',\n",
    "                        'nutrition-score-uk_100g', \n",
    "                        'glycemic-index_100g',\n",
    "                        'water-hardness_100g',\n",
    "                        'ph_100g',\n",
    "                        'collagen-meat-protein-ratio_100g',\n",
    "]\n",
    "for col in not_weight_on_100g_columns :\n",
    "  per_100g_features_list.remove(col)\n",
    "\n",
    "\n",
    "# In order not to interfere with the process, every value over 100g is considered as NaN (so will cound as 0 in the sum).\n",
    "# This is not a problem, as it will be replaced later in the outlier selection process.\n",
    "for col in per_100g_features_list:\n",
    "  df.loc[df[col] > 100, [col]] = np.nan\n",
    "\n",
    "# We select the columns that only are not composed of other columns (the deepest ones)\n",
    "for feature in multi_purpose_function(dict_feature_combinations,1):\n",
    "  per_100g_features_list.remove(feature)\n",
    "\n",
    "# We drop every column that we feel not to fit in the sum for different reasons\n",
    "col_to_drop = [\n",
    "          'maltodextrins_100g', # Composed of glucose and fructose. Don't know where to put it.\n",
    "          'starch_100g', # Disn't find what it is.\n",
    "          'polyols_100g', # Organic component. Isn't a component of the food.\n",
    "          'serum-proteins_100g', # Protein coding gene. Don't know where to put it.\n",
    "          'nucleotides_100g', # Nucleic acid component. Don't know where to put it.\n",
    "          'beta-carotene_100g', # Precursor of the synthesis of vitamin A. We already study vitamin A.\n",
    "          'folates_100g', # Same as B9 vitamine\n",
    "]\n",
    "\n",
    "for col in col_to_drop:\n",
    "  per_100g_features_list.remove(col)\n",
    "\n",
    "# These columns are the deepest ones\n",
    "per_100g_invar_cols = per_100g_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "z471TTwa29Rd",
    "outputId": "8bc6cd75-5ef2-463e-8a44-7eb768e265eb"
   },
   "outputs": [],
   "source": [
    "# Time to do the sums\n",
    "\n",
    "set_index_surcharge = set()\n",
    "\n",
    "for i, colonne_list_variable in enumerate(multi_purpose_function(dict_feature_combinations,2)):\n",
    "  print(f'Summing the depth level {i+1}')\n",
    "  colonne_nom = 'somme_100g_n' + str(i)\n",
    "\n",
    "\n",
    "  # We sum the deepest one with the level 'columns'\n",
    "  colonne_list = per_100g_invar_cols + list(colonne_list_variable)\n",
    "\n",
    "  df[colonne_nom] = df[colonne_list].sum(axis=1)\n",
    "  set_index_surcharge = set_index_surcharge.union(set(df[(df[colonne_nom] > 100) | (df[colonne_nom] < 0)].index))\n",
    "\n",
    "# When the weight is over 100g for 100g of product, we don't know what columns are wrong so we must delete the entire row\n",
    "\n",
    "print(f\"We delete {len(set_index_surcharge)} rows on {df.shape[0]} which makes {round(len(set_index_surcharge)*100/df.shape[0],2)}%.\")\n",
    "print(f\"There will be left {df.shape[0]-len(set_index_surcharge)} rows.\")\n",
    "\n",
    "df.drop(set_index_surcharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anDYsa4yQF0M"
   },
   "source": [
    "## Interesting features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_0D1BZFQOcV"
   },
   "outputs": [],
   "source": [
    "# We gather the most interesting features (columns) and set options for them that will serve later\n",
    "\n",
    "filter_features = pd.DataFrame(data=[\n",
    "           ['fiber_100g',True, 0,50,0,20],\n",
    "          #  ['cholesterol_100g',False],\n",
    "           ['trans-fat_100g',False, 0,1,0,1],\n",
    "           ['calcium_100g',True,0,2,0,2],\n",
    "           ['iron_100g',True,0,0.2,0,0.04],\n",
    "          #  ['energy_100g',True],\n",
    "           ['proteins_100g',True,0,90,0,30],\n",
    "          #  ['salt_100g',False],\n",
    "          #  ['sodium_100g',False],\n",
    "           ['salt_proc_100g',False,0,100,0,20],\n",
    "           ['vitamins_count',True,0,11,0,11]\n",
    "          ],\n",
    "          columns = ['feature', 'shouldIMaximiseIt', 'min_lim', 'max_lim', 'min_lim_arbitrary', 'max_lim_arbitrary'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pIu1NwiezE2"
   },
   "source": [
    "## Artificial features creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "873WkcFEe7sh"
   },
   "source": [
    "We try to create a new feature counting the number of vitamin in the product. We consider that if a vitamin column is not filled for a product, it doesn't contain it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeGdARjYJxRc"
   },
   "outputs": [],
   "source": [
    "vitamin_columns = []\n",
    "for index in data.columns:\n",
    "  if index[0] == 'v' : vitamin_columns.append(index)\n",
    "\n",
    "# Also adding new vitamins that aren't labeled as 'vitamins'\n",
    "\n",
    "# Vitamine B5\n",
    "vitamin_columns.append('pantothenic-acid_100g')\n",
    "# Vitamine B8\n",
    "vitamin_columns.append('biotin_100g')\n",
    "\n",
    "# Create a new column 'vitamins_count' that will count the number of vitamins in the product\n",
    "vitamins_bool_isna = pd.notna(df[vitamin_columns])\n",
    "df['vitamins_count'] = vitamins_bool_isna.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[vitamin_columns[0:5]][:2]\n",
    "test = data[vitamin_columns[0:5]][:2].applymap(lambda x: False if ((np.isnan(x)) or (x == 0)) else True)\n",
    "test['vitamins_count'] = test.sum(axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYD-NWILgCV4"
   },
   "source": [
    "Depending on the product, either the salt or sodium column is filled, or both.\n",
    "However, even if it is about the same nutriment, sodium quantity isn't the same as salt quantity for a product.\n",
    "We can find [here](https://www.wikiwand.com/en/Sodium_chloride) that salt (Sodium Chloride) is composed by 39.34% of sodium and 60.66% of chloride. So `salt = sodium * (100/39.34)`.\n",
    "In order to use this quantity, we must have a column gathering information about salt using one of the two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiC4l-c_iqcn"
   },
   "outputs": [],
   "source": [
    "salt_columns = ['salt_100g', 'sodium_100g']\n",
    "\n",
    "# NaN values in the salt rows\n",
    "rows_where_salt_na = df['salt_100g'].isna()\n",
    "# but filled value in the sodium row\n",
    "rows_where_sodium = df['sodium_100g'].notna()\n",
    "\n",
    "rows_where_must_calculate_salt = df[rows_where_salt_na & rows_where_sodium].index.tolist()\n",
    "\n",
    "def _fill_salt_proc_100g_column(x):\n",
    "  # If there is no salt but sodium, we return the operation, in the otehr case, we simply return the salt value\n",
    "  return x['sodium_100g'] * (100/39.34) if x.name in rows_where_must_calculate_salt else x['salt_100g']\n",
    "\n",
    "df['salt_proc_100g'] = df.apply(lambda x: _fill_salt_proc_100g_column(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4XkDl5QQx1f"
   },
   "source": [
    "## Outliers selection in selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKY9WMexQ1B4"
   },
   "source": [
    "The goal is to select outliers in order to process them by other methods later.\n",
    "Instead of deleting them as soon as we find them, we prefer selecting them for two reasons :\n",
    "1. Allow to keep NaN values (to process them later)\n",
    "2. Allow to keep the information about the outliers and visualize them later\n",
    "\n",
    "For every columns, we determine max and min values that will contain the regular values.\n",
    "\n",
    "Most of the limits are set thanks to [this report](https://fdc.nal.usda.gov/fdc-app.html#/) of the US department of Agriculture that we will consider as reliable. We also put some more extreme and arbitrary limits to make better visualisations. Later, we will be asked wich one we should keep.\n",
    "\n",
    "\n",
    "We already set the limits prior in this notebook when defining the interesting features columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rc99b848c__r"
   },
   "outputs": [],
   "source": [
    "df_without_outliers = df.copy()\n",
    "df_without_outliers_sharp = df.copy()\n",
    "\n",
    "\n",
    "for index, feature in filter_features.iterrows() :\n",
    "  feature_name = feature['feature']\n",
    "\n",
    "  lim_bas_sharp = feature['min_lim_arbitrary']\n",
    "  lim_haut_sharp = feature['max_lim_arbitrary']\n",
    "  lim_bas = feature['min_lim']\n",
    "  lim_haut = feature['max_lim']\n",
    "\n",
    "  conditions = (df_without_outliers[feature_name] > lim_haut) | (df_without_outliers[feature_name] < lim_bas)\n",
    "  conditions_sharp = (df_without_outliers_sharp[feature_name] > lim_haut_sharp) | (df_without_outliers_sharp[feature_name] < lim_bas_sharp)\n",
    "\n",
    "  # Colonnes we will display over the plots\n",
    "  display_columns = [feature_name,'product_name','brands', 'code']\n",
    "\n",
    "  df_without_outliers.loc[conditions, feature['feature']] = np.nan\n",
    "  df_without_outliers_sharp.loc[conditions_sharp, feature['feature']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULR3FTYqV7Zh"
   },
   "source": [
    "## Outliers visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7M-9KnMWV8x-",
    "outputId": "6d58535a-c22a-4277-f45b-c353ed7da984"
   },
   "outputs": [],
   "source": [
    "# We will visualize our outliers using boxplots\n",
    "\n",
    "# Initialise the subplot function using number of rows and columns\n",
    "figure, axis = plt.subplots(filter_features.shape[0], 3, figsize=(20, 20))\n",
    "cols = ['Not cleaned', \"Cleaned\", \"Sharp cleaned\"]\n",
    "figure.suptitle('Visualisation of the outliers', fontsize=20)\n",
    "for ax, col in zip(axis[0], cols):\n",
    "    ax.set_title(col)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "for index, column in filter_features.iterrows():\n",
    "  # Keeping the oultiers\n",
    "  sns.boxplot(ax=axis[index,0], x=df[column['feature']])\n",
    "\n",
    "  # Removing the oultiers\n",
    "  sns.boxplot(ax=axis[index,1], x=df_without_outliers[column['feature']])\n",
    "\n",
    "  # Removing the oultiers with more fine limits\n",
    "  sns.boxplot(ax=axis[index,2], x=df_without_outliers_sharp[column['feature']])\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX1RxKo3VdJg"
   },
   "source": [
    "We select the cleaning limits that we prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnJOdHXLVcbn"
   },
   "outputs": [],
   "source": [
    "# Shoud we use sharp cleaning ?\n",
    "use_sharp_limits = True\n",
    "\n",
    "df = df_without_outliers_sharp if use_sharp_limits else df_without_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFlvZdzVeg3w"
   },
   "source": [
    "## Missing values treatment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3baZM1bep2l"
   },
   "source": [
    "Interesting features alerady have been selected by using their completion rate, we know that we will be able to work with a lot of data for our prediction.\n",
    "And as we cleaned the data, we know that we will work with consitent data.\n",
    "\n",
    "There are several features to fill, so, for each of them (considered as 'target feature'), in order to predict the missing values, we will use the following methods :\n",
    "1. Determine the 5 more correlated features with the target feature\n",
    "2. If all of those features are filled, we make a regression to predict the target feature\n",
    "3. If any of those correlated feature is missing, we chose to use imputation instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPeWhE6XRJUa"
   },
   "outputs": [],
   "source": [
    "def choose_most_related_features(df, target, nb, features):\n",
    "  print('\\nChoose_most_related_features')\n",
    "\n",
    "  if target in features : features.remove(target)\n",
    "\n",
    "  features_list = features + [target]\n",
    "\n",
    "  corr = df[features_list].corr()\n",
    "\n",
    "  corr = corr.drop(target)\n",
    "  correlated_features = []\n",
    "  for i in range(nb):\n",
    "    feature_label = corr[target].abs().idxmax()\n",
    "    print(f\"-- Feature selected n°{i+1} : {feature_label} with corr {round(corr[target][feature_label],3)*100}%\")\n",
    "    correlated_features.append(feature_label)\n",
    "    corr = corr.drop(feature_label)\n",
    "  return correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a precise imputation method, we will use a sample of the dataset corresponding of the same 'group' of the imputed row.\n",
    "e.g. : If we must predict 'fiber_100g' columns for a product in the group 'fruit juice', we will use the sample of the dataset corresponding to the group 'fruit juice' because using the 'meat' group would be incoherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMwIrYW01iIn"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def make_imputation(df, target, method):\n",
    "  print('\\nImputation')\n",
    "  df['pnns_groups_2'] = df['pnns_groups_2'].apply(lambda x : 'nan' if pd.isna(x) else x) # Do this because if not, it is impossible select the nan group\n",
    "\n",
    "  for group2 in df['pnns_groups_2'].unique():\n",
    "    sub_df = df[df['pnns_groups_2'] == str(group2)][target]\n",
    "    print(f\"------ {group2} --> {sub_df.shape[0]} row and {sub_df.isna().values.sum()} imputations found !\")\n",
    "    sub_df[sub_df.isna()]['target_imputed'] = 1\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy=method)\n",
    "    imp_ser = imputer.fit_transform(sub_df.values.reshape(-1, 1))\n",
    "    df.loc[df['pnns_groups_2'] == group2, target] = imp_ser\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b56TG8jer97D"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def make_regression(df, features, target):\n",
    "  print('\\nRegression')\n",
    "  # We split the dataset into two groups, the one where the target is filled (to train the regression) and the other where the target is missing (to make the prediction).\n",
    "  columns_used = features + [target]\n",
    "  train_df = df[columns_used]\n",
    "  train_df = train_df.dropna()\n",
    "  \n",
    "  train_features = train_df[features]\n",
    "  train_target = train_df[target]\n",
    "\n",
    "\n",
    "  predict_df = df[df[target].isna()]\n",
    "\n",
    "  predict_df = predict_df[features].dropna()\n",
    "  if (predict_df.shape[0] == 0):\n",
    "    print('-- Not enough valid features to make any prediction. At least one feature in each prediction row is missing. Will do it by imputation.')\n",
    "  else:\n",
    "    print(f\"-- {predict_df.shape[0]} rows eligible to prediction\")\n",
    "    predict_features = predict_df[features]\n",
    "\n",
    "    X = train_features\n",
    "    y = train_target\n",
    "    regr = linear_model.LinearRegression()\n",
    "    print('---- fitting...')\n",
    "    regr.fit(X, y)\n",
    "    print('---- fitted')\n",
    "    predict_df[target] = regr.predict(predict_features)\n",
    "    df.loc[predict_df.index,target] = predict_df\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEtjNnnUp566",
    "outputId": "765a5062-2c4d-4210-b31c-c076c795d275"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "nb_correlated_features = 5\n",
    "interesting_features = proportion[proportion > seuil].index.tolist()\n",
    "\n",
    "\n",
    "for index, target in enumerate(filter_features['feature'].tolist()):\n",
    "  # On rajoute une colonne flag pour si la target a été imputed ou non.\n",
    "  # Ca sert pour amélriorer la regression.\n",
    "  # Ce flag est reset pour chaque nouvelle target\n",
    "  df['target_imputed'] = 0\n",
    "  \n",
    "  print(f\"\\n\\n_________________________________________________________________\\nFilter n°{index+1} : {target}\")\n",
    "\n",
    "  most_related_features = choose_most_related_features(df, target , nb_correlated_features, interesting_features)\n",
    "  print(f\"\\n{df[target].isna().sum()} targets left to predict\")\n",
    "\n",
    "  regression_df = make_regression(df, most_related_features, target)\n",
    "  print(f\"\\n{regression_df[target].isna().sum()} targets left to impute\")\n",
    "\n",
    "  imputed_df = make_imputation(regression_df, target, 'mean') # Faire une moyenne en prenant la même catégorie de produits\n",
    "\n",
    "  assert imputed_df[target].isna().sum() == 0, f\"imputation failed, there are still missing values in {target}\"\n",
    "\n",
    "  df.loc[:,target] = imputed_df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXYzruelW1rX"
   },
   "source": [
    "# Exploratory analysis of the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "N7xJVN1lXgKA",
    "outputId": "1f611a09-2a33-4126-bbec-59d8604b93b0"
   },
   "outputs": [],
   "source": [
    "corr = df[interesting_features].corr()\n",
    "\n",
    "\n",
    "# To show heatmap\n",
    "fig, axs = plt.subplots(1,1,figsize=(5,5))\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting fact is that ingredient_that_may_be_from_palm oil is greatly correlated with additives.\n",
    "This is not a correlation we exploit later but deserve to be noticed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yj37lbaueBAR",
    "outputId": "2966f576-95d5-4db2-b138-62cb4be7f155"
   },
   "outputs": [],
   "source": [
    "# Analysis of the data repartition on the interesting features columns after cleaning\n",
    "\n",
    "# Initialise the subplot function using number of rows and columns\n",
    "figure, axis = plt.subplots(filter_features.shape[0], 2, figsize=(20, 20))\n",
    "cols = ['Histogramme','Boite à moustache']\n",
    "for ax, col in zip(axis[0], cols):\n",
    "    ax.set_title(col)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "for index, column in filter_features.iterrows():\n",
    "  # histograme\n",
    "  sns.histplot(ax=axis[index,0], x=df[column['feature']])\n",
    "\n",
    "  # Boxplot\n",
    "  sns.boxplot(ax=axis[index,1], x=df[column['feature']])\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnTm8Rugoi49"
   },
   "source": [
    "We can notice that the linear regression has created some outliers (for example with negative values of fiber)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHKfnbacVT4v"
   },
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m5lGwgCrYb4"
   },
   "outputs": [],
   "source": [
    "# Functions definition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Taken from here https://github.com/stenier-oc/realisez-une-analyse-de-donnees-exploratoire/blob/master/functions.py\n",
    "def display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n",
    "    for d1, d2 in axis_ranks: # On affiche les 3 premiers plans factoriels, donc les 6 premières composantes\n",
    "        if d2 < n_comp:\n",
    "\n",
    "            # initialisation de la figure\n",
    "            fig, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "            # détermination des limites du graphique\n",
    "            if lims is not None :\n",
    "                xmin, xmax, ymin, ymax = lims\n",
    "            elif pcs.shape[1] < 30 :\n",
    "                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n",
    "            else :\n",
    "                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n",
    "\n",
    "            # affichage des flèches\n",
    "            # s'il y a plus de 30 flèches, on n'affiche pas le triangle à leur extrémité\n",
    "            if pcs.shape[1] < 30 :\n",
    "                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "                   pcs[d1,:], pcs[d2,:], \n",
    "                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n",
    "                # (voir la doc : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html)\n",
    "            else:\n",
    "                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n",
    "                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n",
    "            \n",
    "            # affichage des noms des variables  \n",
    "            if labels is not None:  \n",
    "                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n",
    "                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n",
    "                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n",
    "            \n",
    "            # affichage du cercle\n",
    "            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n",
    "            plt.gca().add_artist(circle)\n",
    "\n",
    "            # définition des limites du graphique\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliqué\n",
    "            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Cercle des corrélations (F{} et F{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)\n",
    "        \n",
    "def display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n",
    "    for d1,d2 in axis_ranks:\n",
    "        if d2 < n_comp:\n",
    " \n",
    "            # initialisation de la figure       \n",
    "            fig = plt.figure(figsize=(7,6))\n",
    "        \n",
    "            # affichage des points\n",
    "            if illustrative_var is None:\n",
    "                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)\n",
    "            else:\n",
    "                illustrative_var = np.array(illustrative_var)\n",
    "                for value in np.unique(illustrative_var):\n",
    "                    selected = np.where(illustrative_var == value)\n",
    "                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)\n",
    "                plt.legend()\n",
    "\n",
    "            # affichage des labels des points\n",
    "            if labels is not None:\n",
    "                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n",
    "                    plt.text(x, y, labels[i],\n",
    "                              fontsize='14', ha='center',va='center') \n",
    "                \n",
    "            # détermination des limites du graphique\n",
    "            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) * 1.1\n",
    "            plt.xlim([-boundary,boundary])\n",
    "            plt.ylim([-boundary,boundary])\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliqué\n",
    "            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Projection des individus (sur F{} et F{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)\n",
    "\n",
    "def display_scree_plot(pca):\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_dendrogram(Z, names):\n",
    "    plt.figure(figsize=(10,25))\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('distance')\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        labels = names,\n",
    "        orientation = \"left\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b8uxA-GXqO5z",
    "outputId": "802e7f33-54ff-48bc-9d62-844fb18f3f0f"
   },
   "outputs": [],
   "source": [
    "# PCA realisation\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "floatInterrestingFeatures = [col for col in df[interesting_features].columns if df[col].dtypes == 'float64'] \n",
    "data = df[filter_features['feature']]\n",
    "\n",
    "# Selection of the number of PCA components\n",
    "n_comp = min(data.shape[0], data.shape[1]-1)\n",
    "\n",
    "# Centering and Reduction\n",
    "std_scale = preprocessing.StandardScaler().fit(data.values)\n",
    "X_scaled = std_scale.transform(data.values)\n",
    "\n",
    "\n",
    "# PCA calculations\n",
    "pca = decomposition.PCA(n_components=n_comp)\n",
    "\n",
    "\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Cumulated inertia\n",
    "display_scree_plot(pca)\n",
    "\n",
    "# Correlation circles\n",
    "pcs = pca.components_\n",
    "display_circles(pcs, n_comp, pca, [(0,1),(2,3),(4,5)], labels = np.array(data.columns))\n",
    "\n",
    "# # Data projection (doesn't work properly)\n",
    "# X_projected = pca.transform(X_scaled)\n",
    "# display_factorial_planes(X_projected, n_comp, pca, [(0,1),(2,3),(4,5)], labels = np.array(data.index))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf4ACFXOzVxK"
   },
   "source": [
    "# ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NDr_S8B0H2Z"
   },
   "source": [
    "## 1-variable ANOVA : pnns_groups_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gb-_3QI-0I90",
    "outputId": "fe6e21f2-af79-44c3-fcee-6cbe5b2623ab"
   },
   "outputs": [],
   "source": [
    "# Target repartition analisys depending of the group\n",
    "\n",
    "sns.set(font_scale = 2)\n",
    "figure, axis = plt.subplots(filter_features.shape[0], 2, figsize=(40,100))\n",
    "\n",
    "\n",
    "cols = ['Histogramme','Boite à moustache']\n",
    "axis[0, 0].set_title(cols[0],fontsize=20)\n",
    "axis[0, 1].set_title(cols[1],fontsize=20)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "for index, feature in filter_features.iterrows():\n",
    "\n",
    "  \n",
    "  axHist = axis[index, 0]\n",
    "  axBoxP = axis[index, 1]\n",
    "\n",
    "  # On rajoute le titre pour chaque feature\n",
    "  axHist.set_title(feature['feature'],fontsize=40)\n",
    "  axBoxP.set_title(feature['feature'],fontsize=40)\n",
    "\n",
    "  axBoxP.tick_params(axis='x', rotation=90)\n",
    "\n",
    "\n",
    "  # histograme\n",
    "  sns.kdeplot(ax=axis[index,0], data=df,  x = feature['feature'], hue = 'pnns_groups_2')\n",
    "  plt.setp(axis[index,0].get_legend().get_texts(), fontsize='12') # for legend text\n",
    "  plt.setp(axis[index,0].get_legend().get_title(), fontsize='22') # for legend title\n",
    "\n",
    "  # Boxplot\n",
    "  sns.boxplot(ax=axis[index,1], data=df, x='pnns_groups_2', y = feature['feature'])\n",
    "\n",
    "\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVS7XoSXDX_3"
   },
   "source": [
    "## Hypothese redaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_Zgbp0TDkLA"
   },
   "source": [
    "H0 : I think that the variable 'pnns_groups_2' has no influence on the salt rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GH1Dc2VnD8GA",
    "outputId": "0c3e8652-0f1c-4558-f310-e6fac0c98619"
   },
   "outputs": [],
   "source": [
    "%pip install statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "anova_group_salt = smf.ols('salt_proc_100g~pnns_groups_2', data=df).fit()\n",
    "print(anova_group_salt.summary())\n",
    "\n",
    "if sm.stats.anova_lm(anova_group_salt, typ=2)['PR(>F)'][f\"pnns_groups_2\"] >= 0.05 :\n",
    "  print('\\n\\n\\nBy fisher test, the group has no significant influence on salt rate. H0 is rejected.')\n",
    "else:\n",
    "  print('\\n\\n\\nBy fisher test, the group has a significant influence on salt rate. H0 is accepted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGrzQl4rTREf"
   },
   "source": [
    "## 1-variable ANOVA : Seeking for a qualitative variable uncorrelated with the salt rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsNSkvyAL8RK",
    "outputId": "b55faa10-8f75-442e-9e46-347709ea8945"
   },
   "outputs": [],
   "source": [
    "for feature in df.columns :\n",
    "  another_df = df.copy()\n",
    "  if ((len(df[feature].unique()) < 500) & (df[feature].dtypes == 'object') & (len(df[feature].unique()) > 1)):\n",
    "    another_df.loc[df[feature].isna(), feature] = 'nan'\n",
    "    anova_categorie_salt = smf.ols(f'salt_proc_100g~{feature}', data=another_df).fit()\n",
    "    if sm.stats.anova_lm(anova_categorie_salt, typ=2)['PR(>F)'][f\"{feature}\"] >= 0.05 :\n",
    "      print(f\"{feature} has no significant influence on salt rate.\")\n",
    "      print(anova_categorie_salt.summary())\n",
    "      df = another_df.copy() # We keep every modification we made on the 'not influent feature' as we will reuse it later\n",
    "      break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKzGD40vGNqJ"
   },
   "source": [
    "## 2-variable ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzyvT6cSTiDH"
   },
   "source": [
    "We know that the variable 'pnns_groups_2'has a influence on the salt rate, which isn't the case for the variable 'ingredients_from_palm_oil_tags'.\n",
    "\n",
    "So we will do another ANOVA with the two variables in order to know if 'pnns_groups_2' has an influence on the salt rate for each 'pnns_groups_2' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QUykfdCT2zS",
    "outputId": "3d714c92-2301-4a4e-9059-e7e5d637a350"
   },
   "outputs": [],
   "source": [
    "cat1 = 'pnns_groups_2'\n",
    "cat2 = 'ingredients_from_palm_oil_tags'\n",
    "target = 'salt_proc_100g'\n",
    "\n",
    "formula = f'{target}~{cat1}*{cat2}'\n",
    "anova_interactions_salt = smf.ols(formula, data=df).fit()\n",
    "print(anova_interactions_salt.summary())\n",
    "anova_lm = sm.stats.anova_lm(anova_interactions_salt)\n",
    "print(anova_lm)\n",
    "\n",
    "if sm.stats.anova_lm(anova_interactions_salt, typ=2)['PR(>F)'][f\"{cat1}:{cat2}\"] >= 0.05 :\n",
    "  print(f\"\\n\\n\\nInteraction between {cat1} and {cat2} hasn't any noticeable influence on the salt rate using Fisther test.\")\n",
    "else:\n",
    "  print(f\"\\n\\n\\nInteraction between {cat1} and {cat2} has a noticeable influence on the salt rate using Fisther test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7USWHTxhtqg"
   },
   "source": [
    "# Dynamic filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOgby40shyCC"
   },
   "source": [
    "Le but est que l'utilisateur puisse choisir le (ou les critères) qui lui conviennent le plus et rentre un nom de produit. La base de donnée lui permettra de trouver ce qui correspond le mieux à sa demande et lui fournira aussi un histogramme regroupant tous les produits concurrents et leurs avantages.\n",
    "\n",
    "Critères :\n",
    "* fiber_100g\n",
    "* additives_fr\n",
    "* additives_tags\n",
    "* cholesterol_100g\n",
    "* trans-fat_100g\n",
    "* calcium_100g\n",
    "* vitamin-c_100g\n",
    "* iron_100g\n",
    "* vitamin-a_100g\n",
    "* energy_100g\n",
    "* proteins_100g\n",
    "* salt_100g\n",
    "* sodium_100g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCWnEXsMDwVo"
   },
   "source": [
    "## Entrées Utilisateur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR0TLycWQVa7"
   },
   "source": [
    "Parameters are\n",
    "1. Which filters\n",
    "2. What Product\n",
    "3. How many other products to compare\n",
    "4. Which location (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twDEanNQHAhy"
   },
   "outputs": [],
   "source": [
    "def ask_user_parameters():\n",
    "  user_product_name = input('Veuillez entrer un nom de produit :\\n')\n",
    "\n",
    "  print(f\"These are the differents filters you can select.\\nPlease type their number for adding them to the filter list or . for stopping the selection.\")\n",
    "  features_list = {str(index+1):str(feature) for index,feature in enumerate(filter_features['feature'].tolist())}\n",
    "  for index, feature in features_list.items():\n",
    "      print(f\"{index}.  {feature}\")\n",
    "\n",
    "  fin_de_selection = False\n",
    "  filter_set = set()\n",
    "  while fin_de_selection == False:\n",
    "    user_filter = input('Votre filtre ?\\n')\n",
    "    if (user_filter in features_list):\n",
    "      filter_set.add(features_list[user_filter])\n",
    "      print(f\"Voici votre liste de filtres actuels :\\n{filter_set}\")\n",
    "    elif (user_filter == '.'):\n",
    "      fin_de_selection = True\n",
    "    else :\n",
    "      print('Sélection non reconnue. Réessayez s\\'il vous plait.')\n",
    "\n",
    "  nb_res = 'a'\n",
    "  while nb_res.isdigit() == False:\n",
    "    nb_res = input('En plus du meilleur résultat, de combien autre produit voulez-vous voir le comparatif ?\\n')\n",
    "  nb_res = int(nb_res)\n",
    "  \n",
    "  return list(filter_set), user_product_name, nb_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyxpRJYbGj83"
   },
   "outputs": [],
   "source": [
    "def pick_random_parameters(df):\n",
    "\n",
    "  nb_filter = np.random.randint(1,6) # De 1 à 5\n",
    "  filters = np.random.choice(filter_features['feature'].tolist(), nb_filter, replace=False)\n",
    "  top_nb = np.random.randint(3, 11) # De 3 à 10\n",
    "\n",
    "  print(f\"Filters are {filters}\")\n",
    "\n",
    "  # Pick one or more product (with enough products registered)\n",
    "  sub_df_value_counts = df['product_name'].value_counts() > 50\n",
    "  sub_df_value_counts = sub_df_value_counts[sub_df_value_counts.values == True].index\n",
    "\n",
    "  product_name = np.random.choice(sub_df_value_counts, 1)[0]\n",
    "  print(f\"Product name is {product_name}\")\n",
    "\n",
    "  return filters, product_name, top_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yn4ae_iMDzUb",
    "outputId": "132be138-84d8-4485-8c06-cc8a4a08ac00"
   },
   "outputs": [],
   "source": [
    "entree_reconnue = False\n",
    "\n",
    "while entree_reconnue == False:\n",
    "  # user_input = input('Voulez vous entrer les données utilisateur ? (Y/N)')\n",
    "  user_input = 'N'\n",
    "  if (user_input == 'Y'):\n",
    "    entree_reconnue = True\n",
    "    filters, product_name, nb_res = ask_user_parameters()\n",
    "  elif (user_input == 'N'):\n",
    "    entree_reconnue = True\n",
    "    filters, product_name, nb_res = pick_random_parameters(df)\n",
    "  else :\n",
    "    print('Entrée non reconnue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn_H5OtQqXsX"
   },
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWtuPUGF45H_"
   },
   "source": [
    "Le ranking se fait comme suit :\n",
    "1. Pour chaque feature, on effectue un ranking des individus (voir [cette méthode](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rank.html)). __Attention, certaines features doivent être traitées de sorte à maximiser ou minimiser les quantités (ex : protéines et sel).__\n",
    "2. On effectue la somme des ranking de ces features par individu\n",
    "3. Enfin, on rank de nouveau cette somme afin d'avoir un classement. Cette fois-ci, on utilise l'option `method='dense'` afin d'avoir un score de type ordinal.\n",
    "4. Après un trie du meilleur score au moins bon, on peut séléctionner le premier individu (gagnant ainsi que les suivants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "-pNMVN8uqZWn",
    "outputId": "7dcbeb53-9661-4f38-baf1-eb495171f662"
   },
   "outputs": [],
   "source": [
    "selected_df = df\n",
    "\n",
    "# On séléctionne les produits par le nom\n",
    "product_list = selected_df[selected_df['product_name'].str.contains(product_name, case=False, na=False)]\n",
    "\n",
    "list_score_col_label = set()\n",
    "for index, feature in enumerate(filters):\n",
    "  should_maximise = filter_features.loc[filter_features['feature'] == feature,('shouldIMaximiseIt')].iloc[0]\n",
    "  print(f\"feature is {feature} and should I maximise it ? {should_maximise}\")\n",
    "  product_list[feature + '_rank'] = product_list[feature].rank(ascending=should_maximise)\n",
    "  list_score_col_label.add(feature + '_rank')\n",
    "\n",
    "product_list['sum_scores_rank'] = product_list[list_score_col_label].sum(axis=1)\n",
    "product_list['multiple_rank'] = product_list['sum_scores_rank'].rank()\n",
    "best = product_list.nlargest(nb_res+1, 'multiple_rank')\n",
    "product_list_size = product_list.shape[0]\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJH9y2iYekD9"
   },
   "source": [
    "## Affichage des meilleurs produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "G6cEpaQJfMAH",
    "outputId": "b3c35277-5635-4964-9184-d80e6e7cfcf4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "if len(list_score_col_label) > 1:\n",
    "  new_perc_cols = set()\n",
    "  list_score_col_label.add('multiple_rank')\n",
    "\n",
    "  # Pour que ce soit plus lisible dans le graphique, on va noter chaque rank en pourcentage (ex : 6ème sur 10 --> (6/10) * 100 = au dessus de 60% de l'échantillon)\n",
    "  for feature_rank in list_score_col_label:\n",
    "    col_name = feature_rank + '_perc'\n",
    "    best[col_name] = best[feature_rank]*100/product_list_size\n",
    "    new_perc_cols.add(col_name)\n",
    "  \n",
    "else : # Si il n'y a qu'une seule colonne, on préfere afficher la colonne en question plutôt qu'un ranking par rapport aux autres\n",
    "  new_perc_cols = filters\n",
    "\n",
    "# On rajoutera aussi aux brand names le nom du produit\n",
    "best['new_name'] = best['brands'] + '\\n' + best['product_name']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Avec matplotlib\n",
    "# best.plot(x='new_name', y=new_perc_cols, kind='bar')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Avec sns\n",
    "best_sns = best.melt(id_vars=\"new_name\")\n",
    "\n",
    "best_sns = best_sns.drop(best_sns[~best_sns['variable'].isin(new_perc_cols)].index).sort_values('value', ascending=False)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(len(new_perc_cols)*5,5))\n",
    "sns.barplot(x='new_name', y='value', hue='variable', data=best_sns, ax=axs)\n",
    "axs.tick_params(axis='x', rotation=90, labelsize=10)\n",
    "axs.set_title(f\"Ranking of {product_name} by {filters}\",fontsize=20)\n",
    "axs.set_xlabel(\"Product name\",fontsize=20)\n",
    "axs.set_ylabel(\"Ranking (in %)\",fontsize=20)\n",
    "axs.set_ylim(bottom = 0)\n",
    "axs.tick_params()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGGAb9Np5R5F",
    "outputId": "d36f3451-eb53-4282-e28d-7e7b297e14ff"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/10371200/get-image-from-website\n",
    "\n",
    "%pip install requests\n",
    "import requests\n",
    "\n",
    "import os\n",
    "\n",
    "%pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "%pip install bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "%pip instamm urllib\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "def get_picture_from_url(url):\n",
    "  def get_all_images(url):\n",
    "      \"\"\"\n",
    "      Returns all image URLs on a single `url`\n",
    "      \"\"\"\n",
    "      soup = bs(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "      urls = []\n",
    "      for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "          img_url = img.attrs.get(\"src\")\n",
    "          if not img_url:\n",
    "              # if img does not contain src attribute, just skip\n",
    "              continue\n",
    "          # make the URL absolute by joining domain with the URL that is just extracted\n",
    "          img_url = urljoin(url, img_url)\n",
    "          try:\n",
    "              pos = img_url.index(\"?\")\n",
    "              img_url = img_url[:pos]\n",
    "          except ValueError:\n",
    "              pass\n",
    "          # finally, if the url is valid\n",
    "          if bool(urlparse(img_url).netloc) and bool((urlparse(img_url).scheme)):\n",
    "              urls.append(img_url)\n",
    "      return urls\n",
    "\n",
    "  filename = ''\n",
    "  file_name_start = 'front_en.'\n",
    "  file_name_end = '.full.jpg'\n",
    "  imgs = get_all_images(url)\n",
    "  for img in imgs:\n",
    "    if ((file_name_start in img) and (img.endswith(file_name_end))):\n",
    "      # download the body of response by chunk, not immediately\n",
    "      response = requests.get(img, stream=True)\n",
    "      # get the total file size\n",
    "      file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "      # get the file name\n",
    "      filename = os.path.join('./', img.split(\"/\")[-1])\n",
    "      # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "      progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "      with open(filename, \"wb\") as f:\n",
    "          for data in progress.iterable:\n",
    "              # write data read to the file\n",
    "              f.write(data)\n",
    "  if filename == '':\n",
    "    print('\\nAucune image trouvée')\n",
    "  else:\n",
    "    print(f\"\\nfile name = {filename}\")\n",
    "    pictures.append(filename)\n",
    "\n",
    "pictures = []\n",
    "for url in best['url']:\n",
    "  get_picture_from_url(url)\n",
    "\n",
    "for picture in pictures:\n",
    "  import matplotlib.pyplot as plt\n",
    "  img = mpimg.imread(picture)\n",
    "  imgplot = plt.imshow(img)\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projet3Playground.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
